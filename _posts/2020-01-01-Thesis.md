---
layout:     post
title:      Thesis
author:     Vishal Gattani
tags: 		post template
subtitle: Biomimetic Prosthetic Arm through Motion Capture using Microsoft Kinect V2
category:  project1
---
<!-- Start Writing Below in Markdown -->

# Table of Contents

* TOC
{:toc}

# Basic Functionality

- Human upper limb motion captured through Kinect V2 will be transmitted to various actuators that act as joints on the prosthetic arm.
- The captured data, joint coordinates and angles, will be transmitted through a serial port to arduino and dynamixel shield in order to actuate the servos accordingly.
- The data from both, Motion Capture and Inverse Kinematics, will be synthesised and compared to establish if correlation points exist between the experiments.
- Possible control of the prosthetic may also be achieved through Electromyography (EMG).

# Pipeline Stages

## Kinect | Blender 

Before we start, it’s important to note that everything in this post is about the Kinect for Xbox One, what most people call the Kinect Version 2 or the Kinect V2. The Kinect can track up to six skeletons having 25 joints, all at one time.

![](https://miro.medium.com/max/433/1*ai7YNHm4yOWC6wFCjDmdAw.png)


**Description:** 

In order to have the recorded motion and joint coordinates from the Kinect V2, the open source software Blender has the necessary add-ons from Delicode NI Mate required to create your own moiton capture studio. Blender is a free and open source 3D creation suite that supports the entirety of the 3D pipeline — modeling, rigging, animation, simulation, and motion tracking. For more details, visit [Blender](https://www.blender.org/).

**Hardware Components for Motion Capture:**
- Kinect V2
- Kinect Adapter for Windows 10

**Connections:**

![61HddL27qAL](https://user-images.githubusercontent.com/24211929/71519938-99b7d080-28df-11ea-9e4b-81f951ca2516.jpg)


**Software Prerequisites:**
- Kinect for Windows SDK 2.0
- Kinect for Windows Runtime 2.0
- [Blender 2.78](https://download.blender.org/release/)
- [NI MATE FOR WINDOWS V 2.14](https://ni-mate.com/download/)
- [Official plug-ins for NI Mate - Blender 2.79 and Earlier](https://ni-mate.com/download/)


Before we begin to track all the 25 joints, we have to test if our Kinect is working properly. Follow the tutorial [DIY Kinect Motion Capture Studio | Blender](https://www.youtube.com/watch?v=1UPZtS5LVvw) to setup your Kinect with the Blender add-ons.

**Pipeline:**

![pipeline](https://user-images.githubusercontent.com/24211929/71520910-89562480-28e4-11ea-8fc5-a829d93af095.png)

Follow the instructions from the YouTUbe video inorder to setup Blender and editing the rig that will be used to track and record the motion from Kinect.

### Kinect V2
The Kinect’s camera coordinates use the Kinect’s infrared sensor to find 3D points of the joints in space. These are the coordinates to use for joint positioning in 3D projects. For more details, refer to this [article](https://medium.com/@lisajamhoury/understanding-kinect-v2-joints-and-coordinate-system-4f4b90b9df16).

### [Delicode NI Mate](https://ni-mate.com/)

### Blender 2.78

### Experiments | Files

## Blender | Rigging | Arduino (Serial Port)
![blender_track_basic](https://user-images.githubusercontent.com/24211929/71522246-dd173c80-28e9-11ea-98fc-367e11444dc5.png)


**Procedure:**

After following the [YouTube video](https://www.youtube.com/watch?v=1UPZtS5LVvw), in order to start motion tracking, follow the order:
1. Click on NI Mate tab on the Tool menu.
2. Click on the Auto Keyframing button on the bottom.
3. Click on Start in the NI Mate plug-in tab.
4. To track motion, Click on the play button beside the Auto Keyframing button.

To stop recording, Click the pause and toggle the Auto Keyframing button.

The motion is recorded in keyframes and is available for further analysis.

![rec](https://user-images.githubusercontent.com/24211929/71522713-da1d4b80-28eb-11ea-88fd-32ec7a41ac9c.gif)


**Analysis:**

![graph1](https://user-images.githubusercontent.com/24211929/71541961-4a78ab00-2986-11ea-80ed-51835203a56c.gif)

![poserig_anim](https://user-images.githubusercontent.com/24211929/71541962-4a78ab00-2986-11ea-8b6f-9eba24c70114.gif)

### Joint Graphs

![Blender_  D__vishal_blender_alan_bendknee blend  28-12-2019 15_13_23](https://user-images.githubusercontent.com/24211929/71541920-d4744400-2985-11ea-9bae-423ddc0a30aa.png)

### Saving graphs through Python

```python
import pandas as pd
import numpy as np
import bpy
import math
import os
os.system('cls')

# utility function for searching fcurves
def find_fcurve(id_data, path, index=0):
    anim_data = id_data.animation_data
    for fcurve in anim_data.action.fcurves:
        if fcurve.data_path == path and fcurve.array_index == index:
            return fcurve

joints_tracked = []
for i in bpy.data.objects:
    joints_tracked.append(i.name)   
#print(joints_tracked)   
keep_joints = ['Head', 'Left_Ankle', 'Left_Elbow', 'Left_Foot', 'Left_Hand', 'Left_Hip', 'Left_Knee', 'Left_Shoulder', 'Left_Wrist', 'Neck', 'Right_Ankle', 'Right_Elbow', 'Right_Foot', 'Right_Hand', 'Right_Hip', 'Right_Knee', 'Right_Shoulder', 'Right_Wrist', 'Torso','Right_Collar','Left_Collar'] 
only = list(set(keep_joints) & set(joints_tracked))
#print(only,len(only))

fcurves_x_list = []
fcurves_y_list = []
fcurves_z_list = []
objects_fcurves_x_list = []
objects_fcurves_y_list = []
objects_fcurves_z_list = []

for i in only:
    fcurvex = find_fcurve(bpy.data.objects[i], "location", 0)
    fcurvey = find_fcurve(bpy.data.objects[i], "location", 1)
    fcurvez = find_fcurve(bpy.data.objects[i], "location", 2)
    kfp_x = fcurvex.keyframe_points
    kfp_y = fcurvey.keyframe_points
    kfp_z = fcurvez.keyframe_points
    objects_fcurves_x_list.append(kfp_x)
    objects_fcurves_y_list.append(kfp_y)
    objects_fcurves_z_list.append(kfp_z)
    
df_list = []
for i in range(len(objects_fcurves_x_list)):
    print("Creating dataframe: ",only[i])
    df = pd.DataFrame(columns=['frame', 'x', 'y', 'z'])
    for j in range(len(objects_fcurves_x_list[0])):
        l = [objects_fcurves_x_list[i][j].co[0],objects_fcurves_x_list[i][j].co[1],objects_fcurves_y_list[i][j].co[1],objects_fcurves_z_list[i][j].co[1]]
        df = df.append(pd.Series(l, index=['frame', 'x', 'y', 'z']), ignore_index=True)
    df_list.append(df)
#print(type(fcurve),fcurve.id_data,fcurve.keyframe_points)

print("Saving...")
for i in range(len(df_list)):
    df_list[i].to_csv('./data/27-09-2019/1/'+only[i]+'.csv',index=False)
print("Saved.")
```


### Blender 2.78

### Experiments | Files

**Serial Communciation using Simple Arm rig:**

[*Blender Controller using Arduino*](https://github.com/alvaroferran/BlenderController)

```python
import bpy
import math
import time

import sys
import serial
import glob

port=''.join(glob.glob("/dev/ttyUSB*"))
ser = serial.Serial('COM3',9600)
print("connected to: " + ser.portstr)

ob = bpy.data.objects['Armature']
bpy.context.scene.objects.active = ob

bpy.ops.object.mode_set(mode='POSE')

def sendAngles():
	
    bone1=ob.pose.bones['Link1IK']
    bone2=ob.pose.bones['Link2IK']
   
    pb1 = ob.pose.bones.get("Link1IK")
    pb2 = ob.pose.bones.get("Link2IK")

    v1 = pb1.head - pb1.tail
    v2 = pb2.head - pb2.tail

    if pb1 and pb2:
        #print(v1.angle(v2),",",math.degrees(v1.angle(v2)))
        val = math.degrees(v1.angle(v2))
        val = str(int(val))
        print(val)
        ser.write((val).encode('UTF-8'))

def frameChange(passedScene):
	
	sendAngles()
    
bpy.app.handlers.frame_change_pre.append(frameChange)
```

# Documentation

## Introduction

## Literature Review

Papers:

## Design and Implementation

## Experiments and Results

### Header 3

## Styling

**Bold**

*Italics*

***Bold and Italics***

## Lists

1. Item 1

2. Item 2

* Unordered Item

  * Sub Item 1

    1. **Bold** Sub Sub Ordered Item

## Links

[In-Line](https://www.google.com)

[I'm a reference-style link 1][1]

[I'm a reference-style link 1][2]

[1]:https://www.mozilla.org
[2]:http://www.reddit.com

## Images

Hold your pointer clicked over the image to expand the view.

![Description](http://projectpages.github.io/project-pages/img/Logo_Fairy_Tail_right.png)

## Code

Inline `code`.

{% highlight python %}
import numpy as np
def hello_world():
    print('Hello World!'')
{% endhighlight %}

## MathJax

Use MathJax for Math.
$$ A = \pi r^2 $$

## Tables

Here | is | a | row!
|---------|:----------|:----------:|---------:|
is   |Left|  Center  |Right|
a    | cut | it | A
column  | short | B | C

## Quotes

> War does not decide who is *right*, only who is **left**.

## Rule

---

## HTML

Can write the whole post or sections in HTML for very specific needs. [For the advanced user or the code savvy.]

# Advanced Functionality

Head over to the [documentation page](http://projectpages.github.io/ppguide/) for tutorials on some basic html formatting and some extensions you can use for cool stuff like interactive 3D visualizations.

## Color and Alignment

<p align="center">This text is centered.</p>

<p style="color:red">This is a red text with <span style="color:blue">blue</span> and <span style="color:green">green</span> inline text.</p>

# Some Advanced Features

## Data Projector

<embed src="/project-pages/2016/05/02/New-Projector/" height="500px" width="100%">

## STL

<div align="center"><script src="https://embed.github.com/view/3d/vishalgattani/project-pages/gh-pages/stl/
Reachy beta - shoulder.stl"></script></div>


